import streamlit as st
import os
import pandas as pd
import json
import requests
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
from io import BytesIO
from PIL import Image
import numpy as np
import fitz  # PyMuPDF

# Azure Document Intelligence Client Setup
endpoint = "https://nexus-docintelligence.cognitiveservices.azure.com/"
key = "5qnO8ROrY6OIYJ0mD6ucbTdHwpfnTfFJVg4CDqkZtwr9SYRy9jhGJQQJ99ALACYeBjFXJ3w3AAALACOG5INJ"
document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))


# Function for Querying Documents
def query_documents_page():
    st.title("Query Documents")

    query = st.text_input("Ask a question about the documents (e.g., 'Compare amounts for employee benefits and management')")
    if query:
        result = query_documents(query)
        if "```python" in result:
            st.write(result.split("```python")[0])
            code = result.split("```python")[1].split("```")[0]
            # st.code(code, language="python")
            # try:
            #     exec(code, {"px": px})
            #     # Assume the code generates a `fig` object for the Plotly graph
            #     st.plotly_chart(fig)
            # except Exception as e:
            #     st.error(f"Error executing code: {e}")
            try:
                # Create an isolated dictionary for execution
                exec_globals = {"px": px, "go": go}  # Add both Plotly Express and Graph Objects
                exec(code, exec_globals)
            
                # Check if 'fig' is generated by the executed code
                if "fig" in exec_globals:
                    st.plotly_chart(exec_globals["fig"])  # Render the Plotly chart in Streamlit
                else:
                    st.error("No valid Plotly figure was generated in the code.")
            except Exception as e:
                st.error(f"Error executing code: {e}")

        else:
            st.write(result)


# Function for Document Upload and Annotation
def document_upload_page():
    st.title("Nexus DMS: Document Upload and Annotation")

    uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])
    if uploaded_file:
        classification, entities, bboxes, classified_file_path = process_pdf(uploaded_file)
        st.sidebar.subheader("Document Classification")
        st.sidebar.write(classification)
        st.sidebar.subheader("Extracted Entities")
        st.sidebar.json(entities)

        st.subheader("Annotated Image")
        fig = annotate_image_with_plotly(classified_file_path, bboxes)
        st.plotly_chart(fig)


# Main App Function
def main():
    # Display the company logo at the top
    logo_path = "nekko logo black bg.png"  # Update this to the correct path to your logo
    if os.path.exists(logo_path):
        st.image(logo_path, width=200)
        
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox("Choose a page:", ["Document Upload", "Query Documents"])

    if page == "Document Upload":
        document_upload_page()
    elif page == "Query Documents":
        query_documents_page()


# Helper Functions (analyze_pdf, process_pdf, annotate_image_with_plotly, query_documents)
# Azure OCR and Bounding Box Extraction
def analyze_pdf(pdf_file):
    with open(pdf_file, "rb") as file:
        poller = document_analysis_client.begin_analyze_document(model_id="prebuilt-read", document=file)
        result = poller.result()
    content = ""
    bboxes = []
    for page in result.pages:
        width, height = page.width, page.height
        for line in page.lines:
            content += line.content + "\n"
            bboxes.append([(p.x / width, p.y / height) for p in line.polygon])
    return content, bboxes


# PDF Classification and Processing
def process_pdf(uploaded_file):
    pdf_bytes = uploaded_file.read()
    pdf_filename = uploaded_file.name
    temp_file_path = os.path.join("tmp", pdf_filename)
    os.makedirs("tmp", exist_ok=True)
    with open(temp_file_path, "wb") as f:
        f.write(pdf_bytes)

    inv_content, bboxes = analyze_pdf(temp_file_path)
    response = call_gpt4_api(inv_content)
    response = response[7:-3].replace("\n", "").replace("\\n", "")

    try:
        response = json.loads(response)
    except json.JSONDecodeError:
        st.error("Failed to decode JSON response.")

    classification = response.get("category", "Uncategorized")
    destination_folder = os.path.join("Documents", classification)
    os.makedirs(destination_folder, exist_ok=True)
    classified_file_path = os.path.join(destination_folder, pdf_filename)

    # If file exists, overwrite it
    if os.path.exists(classified_file_path):
        os.remove(classified_file_path)
            
    os.rename(temp_file_path, classified_file_path)

    new_row = {"Classified_File_Path": classified_file_path, "File_Content": inv_content}
    file_path = os.path.join("tmp", "nexus_dms.xlsx")
    if os.path.exists(file_path):
        df = pd.read_excel(file_path)
        df = df.drop_duplicates(subset=["File_Content"], keep='last')
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    else:
        df = pd.DataFrame([new_row])
    df.to_excel(file_path, index=False)
    return classification, response, bboxes, classified_file_path


# Annotated Image with Bounding Boxes
def annotate_image_with_plotly(pdf_file_path, bboxes):
    doc = fitz.open(pdf_file_path)
    page = doc.load_page(0)
    pix = page.get_pixmap()
    img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
    fig = px.imshow(img)
    for bbox in bboxes:
        shape = bbox_to_shape(bbox, pix.width, pix.height)
        fig.add_shape(shape)
    return fig  
  
# Function to convert a bounding box to a rectangle shape for Plotly  
def bbox_to_shape(bbox, width, height):  
    x_min = int(bbox[0][0] * width)  
    y_min = int(bbox[0][1] * height)  
    x_max = int(bbox[2][0] * width)  
    y_max = int(bbox[2][1] * height)  
    return {  
        'type': 'rect',  
        'x0': x_min,  
        'y0': y_min,  
        'x1': x_max,  
        'y1': y_max,  
        'line': {  
            'color': 'rgba(255, 0, 0, 0.8)',  
            'width': 2,  
        },  
    }


# GPT-4 API call function for classification  
def call_gpt4_api(prompt):  
    url = "https://oainekko.openai.azure.com/openai/deployments/gpt-4o-nekko/chat/completions?api-version=2024-08-01-preview"  
    # url = "https://oainekko.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview"
    headers = {  
        "Content-Type": "application/json",  
        "api-key": "cacf6dcb95134cdeab048a36fb6232eb"  
    }  
    messages = [  
        {"role": "system", "content": """You are Nexus DMS, The world most advanced Document Management System. Your task is to go through the document contents and do the following:
        1. Assign a `category` to the document. The `category` should be assigned based on the contents of the document and should be as descriptive as possible. Example an invoice charging the company for background verification of new joinees could be categorized as `Employee Management/Invoice/Background Verification`.
        2. Extract relevant entities/ necessary fields from the document.  (depending upon the type of document)
        3. Return the result in structured JSON format. (Note: All Values are to be saved as String)"""},  
        {"role": "user", "content": f"Please find the document contents extracted as text: ```{prompt}```"}  
    ]  
    payload = {  
        "messages": messages,  
        "temperature": 0.7,  
        "max_tokens": 4096  
    }  
    response = requests.post(url, headers=headers, data=json.dumps(payload))  
    response.raise_for_status()  
    print("***************************")
    print(response.json()["choices"][0]["message"]["content"])
    print("***************************")
    return response.json()["choices"][0]["message"]["content"]  

# Querying function to handle document-based queries  
def query_documents(query):  
    # Simulate a response based on the query content.  
    file_path = os.path.join("tmp", 'nexus_dms.xlsx')
    df = pd.read_excel(file_path)
    # df = df.drop_duplicates()
    relevant_data = df.to_json(orient="records")
    
    # Existing prompt modification
    query_prompt = f"""  
    Given the extracted data from the uploaded documents, please respond to the user queries. 
    (Consider only unique entries rejecting duplicates. Duplicates are most likely a bug so its better if it goes unmentioned). 
    
    # Important: Remember these answers are for the leadership team so any valuable additional insights are always appreciated.
    
    # Note : If the customer query requires a bar chart or graph, generate the equivalent Python code with all necessary imports 
    # and ensure the code uses Plotly for interactive graph creation (not Matplotlib).
    
    Extracted data (In JSON Formatting): 
        ```
        {json.dumps(relevant_data)}
        ```
    """

    url = "https://oainekko.openai.azure.com/openai/deployments/gpt-4o-nekko/chat/completions?api-version=2024-08-01-preview"  
    # url = "https://oainekko.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview"
    headers = {  
        "Content-Type": "application/json",  
        "api-key": "cacf6dcb95134cdeab048a36fb6232eb"  
    }  
    messages = [  
        {"role": "system", "content": query_prompt},  
        {"role": "user", "content": query}  
    ]  
    payload = {  
        "messages": messages,  
        "temperature": 0.7,  
        "max_tokens": 4096  
    }  
    response = requests.post(url, headers=headers, data=json.dumps(payload))  
    response.raise_for_status()  
    print("***************************")
    print(response.json()["choices"][0]["message"]["content"])
    print("***************************")
    return response.json()["choices"][0]["message"]["content"]  

if __name__ == "__main__":
    main()
